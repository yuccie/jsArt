---
layout: post
title: 白帽子讲web安全笔记
date: Fri May 10 2019 17:25:48 GMT+0800 (中国标准时间)
---
>写在前面：小时候听到黑客，总感觉很神秘的感觉，多年以后自己做了开发，或多或少的接触一些，但都不太系统，因此这里将。

### **参考资料**

《白帽子讲web安全》、[chrome开发者文档][chromeDevtoolUrl]

---

### **跨站点请求伪造(CSRF)**

---

#### **CSRF简介**

CSRF的全名是`Cross Site Request Forgery`，翻译成中文就是跨站点请求伪造。注意要和`Cross Origin Resource Share`CORS（跨域解决方案之一）区分开。

#### **攻击过程**

浏览器所持有的Cookie有两种，会话型Cookie和持久型Cookie，前者没有设置过期时间，浏览器关闭即销毁。





#### **浏览器的Cookie策略**

浏览器所持有的Cookie有两种，会话型Cookie和持久型Cookie，前者没有设置过期时间，浏览器关闭即销毁。

### **认证与会话管理**

---

#### **who am i**

**`认证`**是最容易理解的一种安全。如果一个系统缺乏认证手段，明眼人都能看出来这是“不安全”的。**最常见的认证方式就是用户名与密码**，但认证的手段却远远不止于此。

很多时候，人们会把**“认证”和“授权”**两个概念搞混，甚至有些安全工程师也是如此。实际上“认证”和“授权”是两件事情，

- 认证的英文是 `Authentication`，授权则是`Authorization`。
- 认证的目的是为了认出用户是谁，而授权的目的是为了决定用户能够做什么。

形象地说，假设系统是一间屋子，持有钥匙的人可以开门进入屋子，那么屋子就是通过“锁和钥匙的匹配”来进行认证的，认证的过程就是开锁的过程。

钥匙在认证过程中，被称为“凭证”`（Credential）`，开门的过程，在互联网里对应的是登录（Login）。可是开门之后，什么事情能做，什么事情不能做，就是“授权”的管辖范围了。

但是有钥匙的人就一定是真正的主人吗？比如钥匙让被人复制了。。。这就出现了安全问题，那如何解决呢？

如何才能准确地判断一个人是谁呢？这是一个哲学问题，在被哲学家们搞清楚之前，我们只能够依据人的不同“凭证”来确定一个人的身份。钥匙仅仅是一个很脆弱的凭证，其他诸如指纹、虹膜、人脸、声音等生物特征也能够作为识别一个人的凭证。**认证实际上就是一个验证凭证的过程**。

如果**只有一个凭证被用于认证，则称为“单因素认证”**；如果有两个或多个凭证被用于认证，则称为“双因素`（Two Factors）`认证”或“多因素认证”。一般来说，多因素认证的强度要高于单因素认证，但是在用户体验上，多因素认证或多或少都会带来一些不方便的地方。

#### **密码的那些事儿**

密码认证方式简单易行，但安全等级较低。而密码强度在密码认证中也是一环，可以参考`OWASP`（开放式Web应用程序安全项目）推荐的最佳实践：

密码长度方面：

- 普通应用要求长度为6位以上；
- 重要应用要求长度为8位以上，并考虑双因素认证。

密码复杂度方面：

- 密码区分大小写字母；
- 密码为大写字母、小写字母、数字、特殊符号中两种以上的组合；
- 不要有连续性的字符，比如 1234abcd，这种字符顺着人的思路，所以很容易猜解；
- 尽量避免出现重复的字符，比如 1111。

除了OWASP推荐的策略外，还需要注意，不要使用用户的公开数据，或者是与个人隐私相关的数据作为密码。比如不要使用QQ号、身份证号码、昵称、电话号码（含手机号码）、生日、英文名、公司名等作为密码，这些资料往往可以从互联网上获得，并不是那么保密。

目前黑客们常用的一种暴力破解手段，**不是破解密码，而是选择一些弱口令**，比如123456，然后猜解用户名，直到发现一个使用弱口令的账户为止。**由于用户名往往是公开的信息，攻击者可以收集一份用户名的字典**，使得这种攻击的成本非常低，而效果却比暴力破解密码要好很多。

密码的保存也有一些需要注意的地方。一般来说，**密码必须以不可逆的加密算法，或者是单向散列函数算法，加密后存储在数据库中。这样做是为了尽最大可能地保证密码的私密性。即使是网站的管理人员，也不能够看到用户的密码**。登录验证密码的过程仅仅是验证用户提交的密码哈希值，与保存在数据库中的是否一致。

目前黑客们广泛使用的一种破解MD5后密码的方法是“彩虹表`（Rainbow Table）`”。

**彩虹表的思路**是收集尽可能多的密码明文和明文对应的MD5值。这样只需要查询MD5值，就能找到该MD5值对应的明文。一个好的彩虹表，可能会非常庞大，但这种方法确实有效。彩虹表的建立，还可以周期性地计算一些数据的MD5值，以扩充彩虹表的内容。

但为了避免密码泄露后，黑客通过彩虹表就查出密码明文，在计算密码明文的哈希值时，增加一个`salt`(常说盐)，`“Salt”`是一个字符串，它的作用是为了增加明文的复杂度，并能使得彩虹表一类的攻击失效。

Salt的使用一般为`MD5(Username+Password+Salt)`,其中，`Salt = abcddcba……`（随机字符串）。Salt应该保存在服务器端的配置文件中，并妥善保管。

#### **多因素认证**

对于很多重要的系统来说，如果只有密码作为唯一的认证手段，从安全上看会略显不足。因此为了**增强安全性，大多数网上银行和网上支付平台都会采用双因素认证或多因素认证**。

比如支付宝的多因素认证方式，除了支付密码外，手机动态口令、数字证书、宝令、支付盾、第三方证书等都可用于用户认证。这些不同的认证手段可以互相结合，使得认证的过程更加安全。密码不再是唯一的认证手段，在用户密码丢失的情况下，也有可能有效地保护用户账户的安全。

#### **Session与认证**

**密码与证书等认证手段，一般仅仅用于登录（Login）的过程**。当登录完成后，用户访问网站的页面，不可能每次浏览器请求页面时都再使用密码认证一次。因此，当**认证成功后，就需要替换一个对用户透明的凭证。这个凭证，就是SessionID**。

在Web应用中，用户登录之后，服务器端通常会建立一个新的Session以跟踪用户的状态。每个Session对应一个标识符SessionID，SessionID用来标识用户身份，一般是加密保存在Cookie中。**有的网站也会将Session保存在Cookie中，以减轻服务器端维护Session的压力**(此时需要服务端对数据做一定处理，类似token)。

最**常见的做法就是把SessionID加密后保存在Cookie**中，因为Cookie会随着HTTP请求头发送，且受到浏览器同源策略的保护。

但是如果`SessionID`在生命周期内别窃取，则等同于账户失窃。此时黑客也不用再攻击登录过程了，因为已经登录了。。。

如果SessionID是保存在Cookie中的，则这种攻击可以称为Cookie劫持。

Cookie泄露的途径有很多，最常见的有`XSS攻击`、`网络Sniff`，以及本地木马窃取。对于`通过XSS漏洞窃取Cookie的攻击，通过给Cookie标记httponly`，可以有效地缓解XSS窃取Cookie的问题。但是其他的泄露途径，比如网络被嗅探，或者Cookie文件被窃取，则会涉及客户端的环境安全，需要从客户端着手解决。

SessionID除了可以保存在Cookie中外，还可以保存在URL中，作为请求的一个参数。但是这种方式的安全性难以经受考验。因为有时候可以通过`referer`泄露URL中的SessionID。

在生成**SessionID时，需要保证足够的随机性**，比如采用足够强的伪随机数生成算法。现在的网站开发中，都有很多成熟的开发框架可以使用。这些成熟的开发框架一般都会提供Cookie管理、Session管理的函数，可以善用这些函数和功能。

[查看更多关于服务端session][serverSessionUrl]

#### **会话期Cookie和持久性Cookie**

`会话期Cookie`是最简单的Cookie：**浏览器关闭之后它会被自动删除，也就是说它仅在会话期内有效。会话期Cookie不需要指定过期时间（Expires）或者有效期（Max-Age）**。**需要注意的是**，有些浏览器提供了会话恢复功能，这种情况下即使关闭了浏览器，会话期Cookie也会被保留下来，就好像浏览器从来没有关闭一样。

针对`会话级Cookie`，当浏览器关闭后，是没有通知服务端的，服务端一般是在30分钟会销毁session。当然如果前端页面一直活着，则后端也会自动更新时间。

和关闭浏览器便失效的会话期Cookie不同，**持久性`Cookie`**可以指定一个特定的过期时间`（Expires）`或有效期`（Max-Age）`

```css
/* 注意过期时间是相对于客户端 */
Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT;
```

参考：[如何在不同平台设置cookie](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Cookies)

#### **第三方Cookie**

每个Cookie都会有与之关联的域（Domain），如果Cookie的域和页面的域相同，那么我们称这个Cookie为第一方`Cookie（first-party cookie）`，如果Cookie的域和页面的域不同，则称之为第三方`Cookie（third-party cookie.）`。一个页面包含图片或存放在其他域上的资源（如图片广告）时，第一方的Cookie也只会发送给设置它们的服务器。通过第三方组件发送的第三方Cookie主要用于广告和网络追踪。

#### **Session Fixation攻击**

什么是`Session Fixation`呢？举一个形象的例子，假设A有一辆汽车，A把汽车卖给了B，但是A并没有把所有的车钥匙交给B，还自己藏下了一把。这时候如果B没有给车换锁的话，A仍然是可以用藏下的钥匙使用汽车的。

这个没有换“锁”而导致的安全问题，就是`Session Fixation`问题。在用户登录网站的过程中，**如果登录前后用户的SessionID没有发生变化，则会存在S`Session Fixation`问题。**

**具体攻击的过程是**，用户X（攻击者）先获取到一个未经认证的SessionID，然后将这个SessionID交给用户Y去认证，**Y完成认证后，服务器并未更新此SessionID的值（注意是未改变SessionID，而不是未改变Session）**，所以X可以直接凭借此SessionID登录进Y的账户。

X如何才能让Y使用这个SessionID呢？如果SessionID保存在Cookie中，比较难做到这一点。但若是SessionID保存在URL中，则X只需要诱使Y打开这个URL即可。

因此，解决**Session Fixation的正确做法是，在登录完成后，重写SessionID**。如果使用sid(`就是SessionID`)则需要重置sid的值；如果使用Cookie，则需要增加或改变用于认证的Cookie值。（一般sid都存储在Cookie里，这里难道没有包含关系？）

#### **Session保持攻击**

一般来说，Session是有生命周期的，当用户长时间未活动后，或者用户点击退出后，服务器将销毁Session。Session如果一直未能失效，会导致什么问题呢？前面提到Session劫持攻击，是攻击者窃取了用户的SessionID，从而能够登录进用户的账户。

但如果攻击者能一直持有一个有效的Session（比如间隔性地刷新页面，以告诉服务器这个用户仍然在活动），而服务器对于活动的Session也一直不销毁的话，攻击者就能通过此有效Session一直使用用户的账户，成为一个永久的“后门”。

但是Cookie有失效时间，Session也可能会过期，攻击者能永久地持有这个Session吗？

一些系统会给`session`设置一个失效时间，当达到失效时间时，`session`就会被销毁，但有一些系统，出于用户体验的考虑，只要这个用户还“活着”，就不会让这个用户的Session失效。从而攻击者可以通过不停地发起访问请求，让Session一直“活”下去。

在Web开发中，网站访问量如果比较大，**维护Session可能会给网站带来巨大的负担**。因此，有一种做法，就是服务器端不维护Session，而把Session放在Cookie中加密保存。当浏览器访问网站时，会自动带上Cookie，服务器端只需要解密Cookie即可得到当前用户的Session了。

这样的Session如何使其过期呢？很多应用都是利用Cookie的Expire标签来控制Session的失效时间，这就给了攻击者可乘之机。因为Cookie的Expire时间是完全可以由客户端控制的，

攻击者甚至可以为Session Cookie增加一个Expire时间，使得原本浏览器关闭就会失效的Cookie持久化地保存在本地，~~变成一个第三方Cookie（third-party cookie）~~，这里应该变为持久性Cookie？。

**如何对抗这种Session保持攻击呢**？

常见的做法是在一定时间后，强制销毁Session。这个时间可以是从用户登录的时间算起，设定一个阈值，比如3天后就强制Session过期。

但强制销毁Session可能会影响到一些正常的用户，还可以选择的方法是当用户客户端发生变化时，要求用户重新登录。比如用户的IP、UserAgent等信息发生了变化，就可以强制销毁当前的Session，并要求用户重新登录。

最后，还需要考虑的是同一用户可以同时拥有几个有效Session。若每个用户只允许拥有一个Session，则攻击者想要一直保持一个Session也是不太可能的。当用户再次登录时，攻击者所保持的Session将被“踢出”

### **访问控制**

---

#### **what can i do**

指出`“认证（Authentication）”与“授权（Authorization）”`的不同。**“认证”解决了“Who am I?”的问题，而“授权”则解决了“What can I do?”的问题**。

权限控制，或者说访问控制，广泛应用于各个系统中。抽象地说，都是某个主体（subject）对某个客体（object）需要实施某种操作（operation），而系统对这种操作的限制就是权限控制。

**在网络中**，为了保护网络资源的安全，一般是**通过路由设备或者防火墙建立基于IP的访问控制**。这种访问控制的“主体”是网络请求的发起方（比如一台PC），“客体”是网络请求的接收方（比如一台服务器），主体对客体的“操作”是对客体的某个端口发起网络请求。这个操作能否执行成功，是受到防火墙ACL(`Access Control List,访问控制列表,是路由器和交换机接口的指令列表，用来控制端口进出的数据包。`)策略限制的。

**在操作系统中**，对文件的访问也有访问控制。此时“主体”是系统的用户，“客体”是被访问的文件，能否访问成功，将由操作系统给文件设置的ACL（访问控制列表）决定。比如在Linux系统中，一个文件可以执行的操作分为“读”、“写”、“执行”三种，分别由 r、w、x表示。这三种操作同时对应着三种主体：文件拥有者、文件拥有者所在的用户组、其他用户。主体、客体、操作这三者之间的对应关系，构成了访问控制列表。

在一个安全系统中，**确定主体的身份是“认证”解决的问题；而客体是一种资源，是主体发起的请求的对象。在主体对客体进行操作的过程中，系统控制主体不能“无限制”地对客体进行操作，这个过程就是“访问控制”**。

在Web应用中，根据访问客体的不同，常见的访问控制可以分为**“基于URL的访问控制”、“基于方法（method）的访问控制”和“基于数据的访问控制”**。

在正常情况下，管理后台的页面应该只有管理员才能够访问。但这些系统未对用户访问权限进行控制，**导致任意用户只要构造出了正确的URL，就能够访问到这些页面**。

在正常情况下，这些管理页面是不会被链接到前台页面上的，搜索引擎的爬虫也不应该搜索到这些页面**。但是把需要保护的页面“藏”起来，并不是解决问题的办法。攻击者惯用的伎俩是使用一部包含了很多后台路径的字典**，把这些“藏”起来的页面扫出来。比如上面的4个案例中，有3个其管理URL中都包含了“admin”这样的敏感词。而“admin”这个词，必然会被收录在任何一部攻击的字典中

在这些案例的背后，其实只需要加上简单的“基于页面的访问控制”，就能解决问题了

#### **垂直权限管理**

访问控制实际上是建立用户与权限之间的对应关系，现在应用广泛的一种方法，就是“基于角色的访问控制（`Role-Based Access Control`）”，简称 RBAC。

`RBAC`事先会在系统中定义出不同的角色，不同的角色拥有不同的权限，一个角色实际上就是一个权限的集合。而系统的所有用户都会被分配到不同的角色中，一个用户可能拥有多个角色，角色之间有高低之分（权限高低）。在系统验证权限时，只需要验证用户所属的角色，然后就可以根据该角色所拥有的权限进行授权了。

#### **水平权限管理**

用户A与用户B可能都属于同一个角色RoleX，但是用户A与用户B都各自拥有一些私有数据，在正常情况下，应该只有用户自己才能访问自己的私有数据。

但是在RBAC这种“基于角色的访问控制”模型下，`系统只会验证用户A是否属于角色RoleX，而不会判断用户A是否能访问只属于用户B的数据DataB`，因此，发生了越权访问。这种问题，我们就称之为“水平权限管理问题”。

相对于垂直权限管理来说，**水平权限问题出在同一个角色上。系统只验证了能访问数据的角色，既没有对角色内的用户做细分，也没有对数据的子集做细分**，因此缺乏一个用户到数据之间的对应关系。由于水平权限管理是系统缺乏一个数据级的访问控制所造成的，因此水平权限管理又可以称之为“基于数据的访问控制”。

一个简单的数据级访问控制，可以考虑使用“用户组（Group）”的概念。比如一个用户组的数据只属于该组内的成员，只有同一用户组的成员才能实现对这些数据的操作。

此外，还可以考虑实现一个规则引擎，将访问控制的规则写在配置文件中，通过规则引擎对数据的访问进行控制。

水平权限管理问题，至今仍然是一个难题——它难以发现，难以在统一框架下解决，在未来也许会有新的技术用以解决此类问题。

#### **OAuth简介**

OAuth是一个在不提供用户名和密码的情况下，授权第三方应用访问Web资源的安全协议。OAuth 1.0于2007年12月公布，并迅速成为了行业标准（可见不同网站之间互通的需求有多么的迫切）。2010年4月，OAuth 1.0正式成为了RFC 5849。

OAuth 与 OpenID 都致力于让互联网变得更加的开放。**OpenID解决的是认证问题，OAuth则更注重授权**。认证与授权的关系其实是一脉相承的，后来人们发现，其实更多的时候真正需要的是对资源的授权。

**OAuth委员会实际上是从OpenID委员会中分离出来的**（2006年12月），OAuth的设计原本想弥补OpenID中的一些缺陷或者说不够方便的地方，但后来发现需要设计一个全新的协议。

常见的应用OAuth的场景，**一般是某个网站想要获取一个用户在第三方网站中的某些资源或服务**。

比如在人人网上，想要导入用户MSN里的好友，在没有OAuth时，可能需要用户向人人网提供MSN用户名和密码。

参考[OAuth解析(阮一峰)](http://www.ruanyifeng.com/blog/2019/04/oauth_design.html)

---

### **应用层拒绝服务攻击**

---

#### **DDOS简介**

`DDOS`又称**分布式拒绝服务**，全称是`Distributed Denial of Service`。`DDOS`是利用合理的请求造成资源过载，导致服务不可用。系统的资源毕竟有限，同一时间处理事情的数量也有限，当有很多没有意义的请求同时涌进来时，就会占用正常请求的处理，当占用的多了，就表现的像似服务宕机。。。

攻击是安全领域中最难解决的问题之一，迄今为止也没有一个完美的解决方案。

#### .**网络层DDOS**

常见的`DDOS`攻击有`SYN flood、UDP flood、ICMP flood`等。其中`SYN flood`是一种最为经典的`DDOS`攻击，其发现于1996年，但至今仍然保持着非常强大的生命力。`SYN flood`如此猖獗是因为它利用了`TCP`协议设计中的缺陷，而`TCP/IP`协议是整个互联网的基础，牵一发而动全身，如今想要修复这样的缺陷几乎成为不可能的事情。

正常情况下的，`TCP`三次握手过程如下：

1. 客户端向服务器端发送一个`SYN`包，包含客户端使用的端口号和初始序列号x；
2. 服务器端收到客户端发送来的`SYN`包后，向客户端发送一个`SYN`和ACK都置位的TCP报文，包含确认号x+1和服务器端的初始序列号y；
3. 客户端收到服务器端返回的`SYN+ACK`报文后，向服务器端返回一个确认号为y+1、序号为x+1的`ACK`报文，一个标准的`TCP`连接完成。

而SYN flood在攻击时，首先伪造大量的源IP地址(发起请求方)，分别向服务器端发送大量的SYN包，此时服务器端会返回SYN/ACK包，因为源地址是伪造的，所以伪造的IP并不会应答，服务器端没有收到伪造IP的回应，会重试3～5次并且等待一个SYN Time（一般为30秒至2分钟），如果超时则丢弃这个连接。攻击者大量发送这种伪造源地址的SYN请求，服务器端将会消耗非常多的资源（CPU和内存）来处理这种半连接，同时还要不断地对这些IP进行SYN+ACK重试。最后的结果是服务器无暇理睬正常的连接请求，导致拒绝服务。

对抗SYN flood的主要措施有SYN Cookie/SYN Proxy、safereset等算法。SYN Cookie的主要思想是**为每一个IP地址分配一个“Cookie”，并统计每个IP地址的访问频率。如果在短时间内收到大量的来自同一个IP地址的数据包，则认为受到攻击，之后来自这个IP地址的包将被丢弃**。

一般来说，大型网站之所以看起来比较能“抗”DDOS攻击，是因为大型网站的带宽比较充足，集群内服务器的数量也比较多。但一个集群的资源毕竟是有限的，在实际的攻击中，DDOS的流量甚至可以达到数G到几十G，遇到这种情况，只能与网络运营商合作，共同完成DDOS攻击的响应。

**注意：**上面说的攻击是**网络层的攻击**，因为此时连接还没有建立（仍然停留在`TCP`网络层）

#### .**应用层DDOS**

应用层DDOS，不同于网络层DDOS，由于发生在应用层，**因此TCP三次握手已经完成，连接已经建立，所以发起攻击的IP地址也都是真实的**。但应用层DDOS有时甚至比网络层DDOS攻击更为可怕，因为今天几乎所有的商业Anti-DDOS设备，只在对抗网络层DDOS时效果较好，而对应用层DDOS攻击却缺乏有效的对抗手段。

应用层`DDOS`什么意思呢？可以从`CC攻击`说起：

`“CC攻击”`的前身是一个叫fatboy的攻击程序，当时黑客为了挑战绿盟的一款反DDOS设备开发了它。绿盟是中国著名的安全公司之一，它有一款叫“黑洞（Collapasar）”的反DDOS设备，能够有效地清洗SYN Flood等有害流量。而黑客则挑衅式地将fatboy所实现的攻击方式命名为：`Challenge Collapasar`（简称CC），意指在黑洞的防御下，仍然能有效完成拒绝服务攻击。

CC攻击的原理非常简单，就是**对一些消耗资源较大的应用页面不断发起正常的请求，以达到消耗服务端资源的目的**。

在Web应用中，查询数据库、读/写硬盘文件等操作，相对都会消耗比较多的资源。比如并发频繁触发数据库查询，查询无法立即完成，资源无法立即释放，会导致数据库请求连接过多，数据库阻塞，网站无法正常打开。

在互联网中充斥着各种搜索引擎、信息收集等系统的爬虫`（spider）`，**爬虫把小网站直接爬死的情况时有发生**，这与应用层DDOS攻击的结果很像。由此看来，应用层DDOS攻击与正常业务的界线比较模糊。

应用层攻击还可以通过将大流量的网站分流一部分完成，比如黑客篡改大流量页面

```html
 <iframe src="http://target" height=0 width=0 ></iframe>
```

这样，只要用户打开了大流量页面，都会对`target`发起一次`HTTP GET`请求，就可能导致`target`拒绝服务。

许多优化服务器性能的方法，或多或少的能缓解这种攻击，比如将使用频率高的数据放在`memcache`中，比查询数据库效率高出很多。但如果黑客想触发耗性能的操作，只需想法命中`memcache`里没有的数据即可，这样便会触发查询数据库。

#### .**IP&Cookie防御**

通过IP地址与Cookie定位一个客户端，如果客户端的请求在一定时间内过于频繁，则对之后来自该客户端的所有请求都重定向到一个出错页面。

从架构上看，这段代码需要放在业务逻辑之前，才能起到保护后端应用的目的，可以看做是一个“基层”的安全模块。

#### .**道高一尺，魔高一丈**

然而这种防御方法并不完美，因为它在客户端的判断依据上并不是永远可靠的。这个方案中有两个因素用以定位一个客户端：**一个是IP地址，另一个是Cookie**。但用户的IP地址可能会发生改变，而Cookie又可能会被清空，如果IP地址和Cookie同时都发生了变化，那么就无法再定位到同一个客户端了。

**如何让IP地址发生变化呢？**使用“代理服务器”是一个常见的做法。在实际的攻击中，大量使用代理服务器或傀儡机来隐藏攻击者的真实IP地址，已经成为一种成熟的攻击模式。攻击者使用这些方法可不断地变换IP地址，就可以绕过服务器对单个IP地址请求频率的限制了。

#### .**几个方面防御**

1. 首先，应用代码要做好性能优化。合理地使用memcache就是一个很好的优化方案，将数据库的压力尽可能转移到内存中。此外还需要及时地释放资源，比如及时关闭数据库连接，减少空连接等消耗。
2. 其次，在网络架构上做好优化。善于利用负载均衡分流，避免用户流量集中在单台服务器上。同时可以充分利用好CDN和镜像站点的分流作用，缓解主站的压力。
3. 最后，也是最重要的一点，实现一些对抗手段，比如限制每个IP地址的请求频率。

#### .**验证码**

验证码是互联网中常用的技术之一，它的英文简称是`CAPTCHA（Completely Automated Public Turing Test to Tell Computers and Humans Apart，全自动区分计算机和人类的图灵测试）`。在很多时候，如果可以忽略对用户体验的影响，那么引入验证码这一手段**能够有效地阻止自动化的重放行为**。

`CAPTCHA`设计的初衷是为了识别人和机器，过于简单和过于复杂都不太好，因此是把双刃剑。

有验证码，就会有验证码破解技术。除了直接利用**图像相关算法识别验证码**外，还可以利用Web实现上可能存在的漏洞破解验证码。

因为**验证码的验证过程，是比对用户提交的明文和服务器端Session里保存的验证码明文是否一致**。所以曾经有验证码系统出现过这样的漏洞：因为验证码消耗掉后SessionID未更新，导致使用原有的SessionID可以一直重复提交同一个验证码。(难道是登陆后SessionID未更新?)

还有的验证码实现方式，是提前将所有的验证码图片生成好，以**哈希过的字符串作为验证码图片的文件名**。在使用验证码时，则直接从图片服务器返回已经生成好的验证码，这种设计原本的想法是为了提高性能。

但这种一一对应的验证码文件名会存在一个缺陷：**攻击者可以事先采用枚举的方式，遍历所有的验证码图片，并建立验证码到明文之间的一一对应关系，从而形成一张“彩虹表”**，这也会导致验证码形同虚设。修补的方式是验证码的文件名需要随机化，满足“不可预测性”原则。

#### .**防御应用层DDOS**

一种比较可靠的方法是让客户端解析一段JavaScript，并给出正确的运行结果。**因为大部分的自动化脚本都是直接构造HTTP包完成的，并非在一个浏览器环境中发起的请求**。因此**一段需要计算的JavaScript，可以判断出客户端到底是不是浏览器**。类似的，发送一个flash让客户端解析，也可以起到同样的作用。但需要注意的是，这种方法并不是万能的，有的自动化脚本是内嵌在浏览器中的“内挂”，就无法检测出来了。

**除了人机识别外，还可以在web server这一层做些防御，其好处是请求尚未到达后端的应用程序里**，因此可以起到一个保护的作用。

在Apache的配置文件中，有一些参数可以缓解DDOS攻击。比如调小Timeout、KeepAliveTimeout值，增加MaxClients值。但需要注意的是，这些参数的调整可能会影响到正常应用，因此需要视实际情况而定。

如果黑客使用了代理服务器、傀儡机进行攻击，该如何有效地保护网站呢？

Yahoo为我们提供了一个解决思路。因为发起**应用层DDOS攻击的IP地址都是真实的，所以在实际情况中，攻击者的IP地址其实也不可能无限制增长**。假设攻击者有1000个IP地址发起攻击，如果请求了10000次，则平均每个IP地址请求同一页面达到10次，攻击如果持续下去，单个IP地址的请求也将变多，但无论如何变，都是在这1000个IP地址的范围内做轮询。

为此Yahoo实现了一套算法，根据IP地址和Cookie等信息，可以计算客户端的请求频率并进行拦截。Yahoo设计的这套系统也是为Web Server开发的一个模块，但在整体架构上会有一台master服务器集中计算所有IP地址的请求频率，并同步策略到每台WebServer上。

#### .**资源耗尽攻击**

`. Slowloris攻击`  
Slowloris是在2009年由著名的Web安全专家RSnake提出的一种攻击方法，其**原理是以极低的速度往服务器发送HTTP请求**。由于Web Server对于**并发的连接数都有一定的上限，因此若是恶意地占用住这些连接不释放，那么Web Server的所有连接都将被恶意连接占用，从而无法接受新的请求，导致拒绝服务**。

在正常的HTTP包头中，是以两个CLRF表示HTTP Headers部分结束的。`Content-Length: 42\r\n\r\n`，但恶意请求只包含一个CLRF。。。

```css
GET / HTTP/1.1\r\n
Host: host\r\n
User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; .NET CLR 1.1.4322; .NET CLR 2.0.503l3; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; MSOffice 12)\r\n
Content-Length: 42\r\n
```

由于Web Server只收到了一个\r\n，因此将认为HTTP Headers部分没有结束，并保持此连接不释放，继续等待完整的请求。此时客户端再发送任意HTTP头，保持住连接即可。

当构造多个连接后，服务器的连接数很快就会达到上限。此类拒绝服务攻击的本质，实际上是对有限资源的无限制滥用。上面案例中，有限资源就是`web server`的连接数。

`. HTTP POST DOS攻击`  
原理是在发送HTTP POST包时，指定一个非常大的Content-Length值，然后以很低的速度发包，比如10～100s发一个字节，保持住这个连接不断开。这样当客户端连接数多了以后，占用住了Web Server的所有可用连接，从而导致DOS。

攻击的本质是针对`MaxClients`有限制的。。。因此，我们可以想到凡是资源有**限制**的地方，都可能发生资源滥用，从而导致拒绝服务。因此对于不可信任的资源使用者需要额外的限制。

另外内存泄露是程序员经常需要解决的问题，而在安全领域，内存泄露则被认为是能够造成拒绝服务攻击的方式。

`. Server Limit DOS攻击`  
Cookie也能造成一种拒绝服务，笔者称之为 Server Limit DOS，并曾在笔者的博客文章中描述过这种攻击。

Web Server对HTTP包头都有长度限制，以Apache举例，默认是8192字节。也就是说，Apache所能接受的最大HTTP包头大小为8192字节（这里指的是Request Header，如果是Request Body，则默认的大小限制是2GB）。如果客户端发送的HTTP包头超过这个大小，服务器就会返回一个4xx错误，提示信息为：

```css
Your browser sent a request that this server could not understand.
Size of a request header field exceeds server limit.
```

因此如果攻击者通过`XSS`攻击，恶意地向客户端写入了一个超长的cookie，在清苦cookie之前，将无法访问cookie所在域的任何页面。(当然要解决的话，需要服务器取消对HTTP头大小的限制)

`. 正则引发的攻击`  
与前面提到的资源耗尽攻击略有不同的是，ReDOS是一种代码实现上的缺陷。我们知道正则表达式是基于`NFA（Nondeterministic Finite Automaton）`的，**它是一个状态机，每个状态和输入符号都可能有许多不同的下一个状态。正则解析引擎将遍历所有可能的路径直到最后。由于每个状态都有若干个“下一个状态”**，因此决策算法将逐个尝试每个“下一个状态”，直到找到一个匹配的。

比如这个正则表达式，当输入只有`aaaax`时，执行过程如下图：

```js
var newReg = /^(a+)+$/;
```

![正则解析字符串时间分析](/jsArt/assets/images/js-theory/regExpTime.png)

也就是2^4种可能，若是16个a，则是2^16种可能。。。这极大地增加了正则引擎解析数据时的消耗。当用户恶意构造输入时，这些有缺陷的正则表达式就会消耗大量的系统资源（比如CPU和内存），从而导致整台服务器的性能下降，表现的结果是系统速度很慢，有的进程或服务失去响应，与拒绝服务的后果是一样的。

---

### **web server 配置安全**

---

#### **简介**

Web服务器是Web应用的载体，如果这个载体出现安全问题，那么运行在其中的Web应用程序的安全也无法得到保障。因此Web服务器的安全不容忽视。

Web服务器安全，考虑的是应用布署时的运行环境安全。这个运行环境包括Web Server、脚本语言解释器、中间件等软件，这些软件所提供的一些配置参数，也可以起到安全保护的作用。

#### **Apache安全**

尽管近年来Nginx、LightHttpd等Web Server的市场份额增长得很快，但Apache仍然是这个领域中独一无二的巨头，互联网上大多数的Web应用依然跑在Apache Httpd上。

Web Server的安全**我们关注两点**：

1. Web Server本身是否安全；
2. Web Server是否提供了可使用的安全功能。

纵观Apache的漏洞史，它曾经出现过许多次高危漏洞。但这些高危漏洞，大部分是由Apache的Module造成的，Apache核心的高危漏洞几乎没有。Apache有很多官方与非官方的Module，默认启动的Module出现过的高危漏洞非常少，大多数的高危漏洞集中在默认没有安装或enable的Module上。

需要注意的是，Apache以root身份或者admin身份运行是一个非常糟糕的决定。这里的admin身份是指服务器管理员在管理机器时使用的身份。这个身份的权限也是比较高的，因为管理员有操作管理脚本、访问配置文件、读/写日志等需求。

#### **Ngnix安全**

近年来Nginx发展很快，它的高性能和高并发的处理能力使得用户在Web Server的选择上有了更多的空间。但从安全的角度来看，Nginx近年来出现的影响默认安装版本的高危漏洞却比Apache要多。

就软件安全本身来看，Nginx与Apache最大的区别在于，检查Apache安全时更多的要关注Module的安全，而Nginx则需要注意软件本身的安全，及时升级软件版本。

- 首先，Nginx的配置非常灵活，在对抗DDOS和CC攻击方面也能起到一定的缓解作用，
- 其次，在Nginx配置中还可以做一些简单的条件判断，比如客户端User-Agent具有什么特征，或者来自某个特定referer、IP等条件，响应动作可以是返回错误号，或进行重定向。


[chromeDevtoolUrl]: https://developers.google.com/web/tools/chrome-devtools/?hl=zh-cn '开发者文档'
[serverSessionUrl]: https://tomcat.apache.org/tomcat-5.5-doc/servletapi/javax/servlet/http/HttpSession.html '服务器端HttpSession'